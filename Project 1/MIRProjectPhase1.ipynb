{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "WTWsrYobJnd3"
   },
   "source": [
    "<div style=\"direction:rtl;line-height:300%;\"><font face=\"XB Zar\" size=5>\n",
    "<div align=center>\n",
    "<font face=\"B Titr\" size=30>\n",
    "<p></p><p></p>\n",
    "به نام خدا\n",
    "<p></p>\n",
    "</font>\n",
    "<p></p>\n",
    "<img src=\"Images/sharif.png\" width=\"25%\">\n",
    "<font color=blue>\n",
    "<br>\n",
    "درس بازیابی پیشرفته اطلاعات\n",
    "<br>\n",
    "مدرس: دکتر سلیمانی\n",
    "</font>\n",
    "<p></p>\n",
    "<font color=green>\n",
    "فاز اول پروژه - سیستم بازیابی اطلاعات داده‌های ویکی‌پدیای فارسی\n",
    "</font>\n",
    "<p></p>\n",
    "<font color=#FF7500>\n",
    "بهار ۹۹\n",
    "<br>\n",
    "دانشگاه صنعتی شریف\n",
    "<br>\n",
    "دانشکده مهندسی کامپیوتر\n",
    "</font>\n",
    "</div>\n",
    "</font>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "BXCFFyc8Jnd7"
   },
   "source": [
    "<div style=\"direction:rtl;line-height:300%;text-align:justify;\" align=\"justify\"><font face=\"XB Zar\" size=5>\n",
    "<font color=red size=7>\n",
    "<p></p>\n",
    "<div align=center>مقدمه </div>\n",
    "</font>\n",
    "<hr>\n",
    "در فاز اول پروژه درس بازیابی پیشرفته اطلاعات، شما باید سیستم بازیابی اطلاعات را برای مجموعه داده‌های ویکی پدیای فارسی پیاده سازی کنید. بدین صورت که مجموعه داده‌هایی که در اختیارتان قرار داده شده را پس از پردازش اولیه و نمایه‌سازی، آماده جستجو عبارات در آن کنید. سعی شده‌است که امکانات خواسته شده در این سیستم متناسب با جست‌وجو‌های کاربردی بر روی داده‌ها باشد.\n",
    "<br>\n",
    "پروژه از ۴ بخش تشکیل شده،‌ بخش اول آن آماده‌سازی اولیه داده‌هاست. پیشنهاد می شود برای پیاده‌سازی این بخش از کتابخانه هضم که توضیحات استفاده از آن در \n",
    "<a href=\"http://www.sobhe.ir/hazm/\">این صفحه</a>\n",
    "آمده است، استفاده کنید. بخش دوم، طراحی و پیاده‌سازی نمایه‌ساز برای داده‌هاست که با گرفتن داده‌های ورودی، نمایه‌ها و داده‌ساختار‌های مورد نیاز برای جستجو اسناد و دیگر نیازمندی‌های سیستم را تولید می‌کند. در بخش سوم می‌بایست امکان جستجو بر روی داده‌ساختار خروجی بخش قبلی را براساس مدل فضای برداری فراهم کنید. در این قسمت عبارت مورد جستجو در صورت دارا بودن غلط املایی باید اصلاح شود. در بخش آخر نیز با استفاده از پرسمان‌ها و اسنادی که به عنوان اسناد مرتبط به آن پرسمان معرفی شده، می‌بایست سیستم بازیابی خود را با استفاده از ۴ معیار ذکر شده در این بخش\n",
    "ارزیابی کنید.\n",
    "<br>\n",
    "در این دفترچه جوپیتر برای هر یک از چهار بخش پروژه، قسمت مجزایی در نظر گرفته شده‌است. شما باید کدهای خود را طوری بزنید که این بخش‌ها طبق توضیح به تفضیل آمده در هر بخش، به درستی کار کنند. کد‌های خود را می‌توانید در بخش‌های اضافه شده توسط خودتان در همین دفترچه جوپیتر بنویسید یا فایل‌های پایتون مربوط به پیاده‌سازی خود را در کنار دفترچه گذاشته و در بخش‌های مختلف این دفترچه بااستفاده از \n",
    "import\n",
    "مناسب از کد‌هایتان استفاده کنید.\n",
    "<br>\n",
    "در نهایت توجه کنید که دو بخش از این فاز پروژه به عنوان قسمت امتیازی برای شما در نظر گرفته شده. در این سند، بخش‌های امتیازی با علامت (*امتیازی*) مشخص شده‌اند. هر کدام از این بخش ها 10 نمره دارند.\n",
    "</font></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "3-nJxAxdJnd8"
   },
   "source": [
    "<div style=\"direction:rtl;line-height:300%;text-align:justify\"><font face=\"XB Zar\" size=5>\n",
    "<font color=red size=7>\n",
    "<p></p>\n",
    "<div align=center>مجموعه دادگان</div>\n",
    "</font>\n",
    "<hr>\n",
    "مجموعه دادگان مورد استفاده در این پروژه از جمع آوری اطلاعات موجود در صفحات ویکی پدیای فارسی به وجود آمده است.\n",
    "این مجموعه اسناد از دو بخش تشکیل شده است\n",
    ".\n",
    "<br>\n",
    "بخش اول که در فایل \n",
    "Persian.xml\n",
    "آمده است، شامل ۱۵۰۰ سند می‌باشد.\n",
    "هر سند شامل شناسه\n",
    "(id)،\n",
    "عنوان\n",
    "(title)،   \n",
    "و متن \n",
    "(text)\n",
    "است.\n",
    "بخش دوم که در پوشه‌ی \n",
    "queries\n",
    "آمده‌است، شامل تعدادی پرسمان است که برای سنجش سیستم‌ پیاده سازی شده‌ی شما مورد استفاده قرار خواهد گرفت.\n",
    "بخش سوم که در پوشه‌ی\n",
    "relevance\n",
    "آمده‌است،\n",
    "شامل یک فایل است که شناسه سند‌های مرتبط با هر پرسمان در آن آمده‌است.\n",
    "</font></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "avT4ky8EJnd-"
   },
   "source": [
    "<div style=\"direction:rtl;line-height:300%;text-align:justify;\" align=\"justify\" ><font face=\"XB Zar\" size=5>\n",
    "<font color=red size=7>\n",
    "<p></p>\n",
    "<div align=center>(10 نمره) بخش اول: آماده‌سازی اولیه داده‌ها</div>\n",
    "</font>\n",
    "<hr>\n",
    "هدف از این بخش اعمال عملیات متنی اولیه بر روی متن خام ورودی است تا کلمات به شکل مناسب برای قرارگیری در نمایه استخراج شوند. برای تسهیل این بخش شما می‌توانید از توابع کتابخانه‌ی هضم که توضیح استفاده از آن در \n",
    "<a href=\"http://www.sobhe.ir/hazm/\">این صفحه</a>\n",
    "آمده است استفاده‌ نمایید. همین طور در صورت نیاز به توضیحات بیشتر در خصوص این کتاب‌خانه می‌توانید به توضیحات مربوط به پروژه‌ی سه سال قبل از طریق\n",
    "<a href=\"http://ce.sharif.edu/courses/95-96/1/ce324-1/assignments/files/assignDir/MIR_Project1.pdf\">این صفحه</a>\n",
    "مراجعه کنید.\n",
    "<br>\n",
    "<br>\n",
    "عملیات مورد انتظار:\n",
    "<ol>\n",
    "    <li>\n",
    "یکسان‌سازی متن: یکی از عملیات مهم در پردازش متون به خصوص در زبان فارسی این عملیات\n",
    "است که شامل یکسان‌سازی استفاده از فاصله و نیم‌فاصله و نحوه‌ی شکستن یا ادغام کلمات و ... است.  به طور مثال، یک مورد از این یکسان‌سازی‌ها نحوه‌ی قرار گیری حرف جمع «ها» در انتهای کلمات جمع است که می‌تواند بدون فاصله چسبیده به کلمه، با یک فاصله‌ی کامل و یا با نیم‌فاصله\n",
    "پس از کلمه بیاید (کتابها، کتاب ها، کتاب‌ها)\n",
    "    </li>\n",
    "    <li> \n",
    "جدا کردن کلمات یک جمله: واحد متن مورد استفاده‌ی ما در ساخت نمایه و همین طور جست‌وجو در یک سیستم اطلاعاتی کلمات هستند. بنابر این جملات ورودی را باید بتوانیم به کلمات آن بشکنیم  و عملیات مورد نیاز را بر روی کلمات انجام دهیم.\n",
    "    </li>\n",
    "    <li>\n",
    "حذف علائم نگارشی: علائم نگارشی مانند نقطه، ویرگول، و ... باید از درون اسناد حذف شوند تا درون نمایه و جست‌وجو‌ها تاثیر نگذارند.\n",
    "    </li>\n",
    "<li>\n",
    "بازگرداندن کلمات به ریشه: عملیات دیگری که روی کلمات متن صورت میگیرد عمل بازگردانی به\n",
    "ریشه\n",
    "(stemming)\n",
    "است تا کلماتی که از یک ریشه هستند همگی یک کلمه به حساب بیاید.\n",
    "    </li>\n",
    "</ol>\n",
    "</font></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "jzEhdYtzJnd_"
   },
   "source": [
    "<div style=\"direction:rtl;line-height:300%;text-align:justify;\" align=\"justify\"><font face=\"XB Zar\" size=5>\n",
    "در این بخش که برای آماده‌سازی اولیه متن داده‌هاست، تابع \n",
    "prepare_text\n",
    "باید طوری بر روی متن ورودی با نام\n",
    "raw_text\n",
    "عمل کند که\n",
    "عملیات‌های مورد انتظار ذکر شده روی متن انجام شود و متن آماده‌شده به عنوان خروجی تابع برگردانده شود. \n",
    "</font>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import unicode_literals\n",
    "from hazm import *\n",
    "from xml.dom import minidom\n",
    "import json\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "fZ98SbQMJneB"
   },
   "outputs": [],
   "source": [
    "def replace_bad_chars(s):\n",
    "    bad_chars = [\"!\", \":\", \"'\",\"؟\",\"*\",\"#\",\"(\",\")\",\"{\",\"}\",\"[\",\"]\",\"،\",\"؛\",\";\",\"+\",\"=\",\"-\",\"_\",\"|\"]\n",
    "    for c in bad_chars:\n",
    "        s = s.replace(c, \" \")\n",
    "    return s\n",
    "\n",
    "def prepare_text(raw_text):\n",
    "    normalizer = Normalizer()\n",
    "    raw_text = normalizer.normalize(raw_text)\n",
    "    sentences = sent_tokenize(raw_text)\n",
    "    words = []\n",
    "    stemmer = Stemmer()\n",
    "    for s in sentences:\n",
    "        s = replace_bad_chars(s)\n",
    "        temp = word_tokenize(s)\n",
    "        for x in temp:\n",
    "            if len(x) > 1:\n",
    "                words.append(stemmer.stem(x))\n",
    "    return words\n",
    "\n",
    "# print('Enter text:')\n",
    "# raw_text = input()\n",
    "#print(prepare_text('سلام'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "collapsed": true,
    "id": "mlwmeLaQJneG"
   },
   "source": [
    "<div style=\"direction:rtl;line-height:300%;text-align:justify;\" align=\"justify\"><font face=\"XB Zar\" size=5>\n",
    "<font color=red size=7>\n",
    "<p></p>\n",
    "<div align=center> (30 نمره) بخش دوم: ساخت نمایه</div>\n",
    "</font>\n",
    "<hr>\n",
    "در این بخش شما باید نمایه‌گذاری‌های مورد نیاز برای بخش جست‌وجو را انجام دهید. تمامی نمایه‌ها باید به صورت پویا باشند به این معنی که با حذف و یا اضافه کردن سندی در طول اجرای برنامه، سند از نمایه حذف شده و یا به آن اضافه شود. \n",
    "<br>\n",
    "شرح نمایه‌های مورد انتظار:\n",
    "<br>\n",
    "<ol>\n",
    "<li>\n",
    "با توجه به مواردی که در بخش بعد می‌آید و نیاز به جست‌وجو‌ی مجزا و با امتیازدهی متفاوت بر روی بخش‌های مختلف سند مثل عنوان یا متن آن، در این قسمت بایستی نمایه‌ی مناسب برای امکان جست‌وجو‌ در بخش‌های مختلف را پیاده‌سازی کنید. با استفاده از نمایه‌ی ساخته‌شده باید بتوان شماره تمامی اسنادی که یک کلمه در آن آمده است و همچنین همه جایگاه‌های این کلمه در هر بخش هر سند را پیدا کرد. انتخاب داده‌ساختار مناسب برای ذخیره نمایه بر عهده خودتان است\n",
    "(البته روش استفاده شده باید مبتنی بر موارد معرفی شده در کلاس باشد.).\n",
    "همچنین باید قادر باشید نمایه‌ها را در فایلی ذخیره کرده و از فایل ذخیره شده بازیابی کنید\n",
    "</li>\n",
    "<li>\n",
    "(*امتیازی*)\n",
    "نمایه‌ی \n",
    "Bigram: \n",
    "با استفاده از این نمایه می‌توان با دادن یک \n",
    "Bigram\n",
    "(ترکیب‌های دو حرفی) \n",
    "تمامی کلمات موجود در لغتنامه که این ترکیب در آنها موجود است را دریافت کرد. این نمایه برای قسمت اصلاح پرسمان که در بخش بعد توضیح داده خواهد شد، مورد استفاده قرار خواهد گرفت. توجه کنید که با حذف یک سند، تمامی کلمات موجود در آن از لغتنامه حذف نمی‌شوند زیرا ممکن است که آن کلمه در سند دیگری نیز آمده باشد. حذف یک کلمه را در صورتی انجام دهید که لیست آن در نمایه‌ی قسمت قبل خالی شده باشد.\n",
    "</li>\n",
    "</ol>\n",
    "</font></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "vjpoyRv9JneH"
   },
   "source": [
    " < d (30 نمره)   iv style=\"direction:rtl;line-height:300%;text-align:justify;\" align=\"justify\"><font face=\"XB Zar\" size=5>\n",
    "این بخش مربوط به ساخت نمایه‌هاست. تابع \n",
    "construct_indexes\n",
    "با گرفتن مسیر مجموعه‌داده‌ها\n",
    "اقدام به ساختن دو نمایه‌ی شرح داده‌شده می‌کند.\n",
    "</font>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "X_akK-pvJneI"
   },
   "outputs": [],
   "source": [
    "def add_to_dict(w, type, doc, pos):\n",
    "    if w in main_index:\n",
    "        if doc in main_index[w]:\n",
    "            if type in main_index[w][doc]:\n",
    "                main_index[w][doc][type].append(pos)\n",
    "            else:\n",
    "                main_index[w][doc][type] = [pos]\n",
    "        else:\n",
    "            main_index[w][doc] = {type:[pos]}\n",
    "    else:\n",
    "        main_index[w] = {doc:{type:[pos]}}\n",
    "\n",
    "\n",
    "def add_to_bigram(w):\n",
    "    for i in range(len(w) - 1):\n",
    "        if w[i:i+2] in bigram_index:\n",
    "            bigram_index[w[i:i+2]].add(w)\n",
    "        else:\n",
    "            bigram_index[w] = {w}\n",
    "            \n",
    "\n",
    "def construct_positional_indexes(docs_path):\n",
    "    my_doc = minidom.parse(docs_path)\n",
    "    pages = my_doc.getElementsByTagName('page')\n",
    "    titles = [pages[i].childNodes[1].firstChild.data for i in range(len(pages))]\n",
    "    ids = [int(pages[i].childNodes[5].firstChild.data) for i in range(len(pages))]\n",
    "    texts = my_doc.getElementsByTagName('text')\n",
    "    for i in range(len(pages)):\n",
    "        words = prepare_text(titles[i])\n",
    "        for j in range(len(words)):\n",
    "            add_to_dict(words[j], \"title\", ids[i], j+1)\n",
    "            add_to_bigram(words[j])\n",
    "        words = prepare_text(texts[i].firstChild.data)\n",
    "        for j in range(len(words)):\n",
    "            add_to_dict(words[j], \"text\", ids[i], j+1)\n",
    "            add_to_bigram(words[j])\n",
    "    return ids;\n",
    "\n",
    "main_index = dict()\n",
    "bigram_index = dict()\n",
    "doc_ids = set(construct_positional_indexes('project1_data\\\\data\\\\Persian.xml'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "51VMBVg3JneM"
   },
   "source": [
    "<div style=\"direction:rtl;line-height:300%;text-align:justify;\" align=\"justify\"><font face=\"XB Zar\" size=5>\n",
    "این بخش برای مشاهده \n",
    "posting list\n",
    "یک کلمه و جایگاه‌های کلمه در هر بخش سند (عنوان و متن) است. تابع\n",
    "get_posting_list\n",
    "با گرفتن\n",
    "word\n",
    "به عنوان کلمه ورودی، یک دیکشنری به عنوان خروجی بر می‌گرداند که کلید‌های دیکشنری شناسه سند‌هایی است که کلمه در آن وجود داشته‌است.\n",
    "    برای هر شناسه سند آمده در کلید‌های دیکشنری، یک دیکشنری به عنوان مقدار وجود خواهد داشت که کلید‌های آن می‌تواند \n",
    "title\n",
    "و\n",
    "text\n",
    "باشد که جایگاه‌های آمدن کلمه در بخش‌های عنوان و متن به صورت لیست به عنوان مقدار هر یک از این کلید‌ها می‌آید. به طور مثال اگر یک کلمه مثل «سلام» در سند‌۱۰ در جایگاه ۲ عنوان و جایگاه‌های ۴ و ۸ متن و در سند ۲۹ در جایگاه ۱۹ متن آمده باشد دیکشنری به صورت آمده در قطعه کد زیر خواهد بود\n",
    "</font>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "VLnvKnQ0JneN"
   },
   "outputs": [],
   "source": [
    "def get_posting_list(word):\n",
    "    if word in main_index:\n",
    "        return main_index[word]\n",
    "    else:\n",
    "        return \"None\"\n",
    "\n",
    "#get_posting_list('اروپایی')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "__qCYhAsJneS"
   },
   "source": [
    "<div style=\"direction:rtl;line-height:300%;text-align:justify;\" align=\"justify\"><font face=\"XB Zar\" size=5>\n",
    "(*امتیازی*)\n",
    "این بخش برای مشاهده تمام کلماتی است که دارای یک دوحرفی خاص درون خود هستند. تابع \n",
    "get_words_with_bigram\n",
    "یک ورودی به عنوان\n",
    "bigram\n",
    "می‌گیرد و تمام کلماتی را که دارای این دو حرفی هستند به عنوان خروجی بر می‌گرداند.\n",
    "</font>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "p6qxX9KAJneT"
   },
   "outputs": [],
   "source": [
    "def get_words_with_bigram(bigram):\n",
    "    if bigram in bigram_index:\n",
    "        return bigram_index[bigram]\n",
    "    else:\n",
    "        return \"None\"\n",
    "\n",
    "#get_words_with_bigram('لا')\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "_Rz7JnPLJneY"
   },
   "source": [
    "<div style=\"direction:rtl;line-height:300%;text-align:justify;\" align=\"justify\"><font face=\"XB Zar\" size=5>\n",
    "این بخش برای اضافه کردن یک سند به نمایه‌ها است.\n",
    "تابع\n",
    "add_document_to_indexes\n",
    "با گرفتن مسیر پوشه داده‌ها و یک شناسه،\n",
    "در صورت نبود آن سند در نمایه‌ها، آن را به نمایه‌ها اضافه می‌کند.\n",
    "</font>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "z3V8mUFHJneZ"
   },
   "outputs": [],
   "source": [
    "def add_document_to_indexes(docs_path, doc_num):\n",
    "    if doc_num in doc_ids:\n",
    "        return \"doc is already in index\"\n",
    "    my_doc = minidom.parse(docs_path)\n",
    "    title = my_doc.getElementsByTagName(\"title\")[0].firstChild.data\n",
    "    text = my_doc.getElementsByTagName(\"text\")[0].firstChild.data\n",
    "    words = prepare_text(title)\n",
    "    for j in range(len(words)):\n",
    "        add_to_dict(words[j], \"title\", doc_num, j+1)\n",
    "        add_to_bigram(words[j])\n",
    "    words = prepare_text(text)\n",
    "    for j in range(len(words)):\n",
    "        add_to_dict(words[j], \"text\", doc_num, j+1)\n",
    "        add_to_bigram(words[j])\n",
    "    doc_ids.add(doc_num)\n",
    "\n",
    "#add_document_to_indexes('project1_data\\\\data\\\\new_doc.xml', 20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "fI62QI7FJnec"
   },
   "source": [
    "<div style=\"direction:rtl;line-height:300%;text-align:justify;\" align=\"justify\"><font face=\"XB Zar\" size=5>\n",
    "این بخش برای حذف کردن یک سند از نمایه است.\n",
    "تابع\n",
    "delete_document_from_indexes\n",
    "با گرفتن مسیر پوشه داده‌ها و یک شناسه سند، آن سند را از نمایه‌ها حذف می‌کند.\n",
    "در صورتی که پس از حذف یک سند، \n",
    "یک کلمه دیگر در بین محتوای سند‌ها وجود نداشته‌باشد، آن کلمه باید از دیکشنری نمایه‌ی اصلی \n",
    "به طور کامل حذف شود.\n",
    "</font>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "DJ_8rPAxJneg"
   },
   "outputs": [],
   "source": [
    "def delete_document_from_indexes(docs_path, doc_num):\n",
    "    deleted_words = []\n",
    "    for n in main_index:\n",
    "        if doc_num in main_index[n]:\n",
    "            del main_index[n][doc_num]\n",
    "            if len(main_index[n]) == 0:\n",
    "                deleted_words.append(n)\n",
    "    for n in deleted_words:\n",
    "        del main_index[n]\n",
    "        for i in range(len(n) - 1):\n",
    "            if n in bigram_index[n[i:i+2]]:\n",
    "                bigram_index[n[i:i+2]].remove(n)\n",
    "                if len(bigram_index[n[i:i+2]]) == 0:\n",
    "                    del bigram_index[n[i:i+2]]\n",
    "    doc_ids.remove(doc_num);\n",
    "#delete_document_from_indexes('data/wiki', 20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "QL-19qvrJnek"
   },
   "source": [
    "<div style=\"direction:rtl;line-height:300%;text-align:justify;\" align=\"justify\"><font face=\"XB Zar\" size=5>\n",
    "این بخش برای ذخیره‌سازی نمایه‌ی اول است\n",
    "و نیازی به ذخیره‌سازی نمایه \n",
    "Bigram نیست \n",
    ".\n",
    "تابع \n",
    "save_index\n",
    "گرفتن مسیر فایل ذخیره کردن نمایه با نام \n",
    "destination\n",
    "نمایه ساخته‌شده را در این مسیر ذخیره می‌کند.\n",
    "</font>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "UestOXpOJnel"
   },
   "outputs": [],
   "source": [
    "def save_index(destination):\n",
    "    data = json.dumps(main_index)\n",
    "    f = open(destination,\"w\")\n",
    "    f.write(data)\n",
    "    f.close()\n",
    "\n",
    "#save_index('project1_data\\\\data\\\\index_backup.json')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "dPL51GrmJnep"
   },
   "source": [
    "<div style=\"direction:rtl;line-height:300%;text-align:justify;\" align=\"justify\"><font face=\"XB Zar\" size=5>\n",
    "این بخش برای بارگذاری نمایه از یک فایل است. تابع \n",
    "load_index\n",
    "با گرفتن مسیر فایل ذخیره شده نمایه با نام \n",
    "source\n",
    "نمایه را از این فایل بارگذاری می‌کند.\n",
    "</font>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ahonOMFMJneq"
   },
   "outputs": [],
   "source": [
    "def load_index(source):\n",
    "    f = open(source)\n",
    "    index = json.load(f)\n",
    "    f.close()\n",
    "    return {k:{int(r):v for r,v in index[k].items()} for k in index}\n",
    "#main_index = load_index('project1_data\\\\data\\\\index_backup.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============ testing 'prepare_text' =============================================\n",
      "prepared text is : ['کتاب', 'مناسب', 'نوشته_شوند', 'در', 'راستا', 'ارتقا', 'سطح', 'آموز', 'کشور', 'تلاش', 'زیاد', 'صور', 'می\\u200cگیرد'] with length: 13\n",
      "\n",
      "============ testing 'get_posting_list' =========================================\n",
      "number of ocurrences of the word فکری  in documents =  169\n",
      "docs with the word: [3014, 3099, 3103, 3119, 3197, 3217, 3229, 3359, 3373, 3404, 3429, 3654, 3699, 3776, 3777, 3798, 3826, 3879, 3897, 3938, 4002, 4062, 4248, 4275, 4321, 4335, 4382, 4388, 4391, 4398, 4400, 4460, 4636, 4650, 4671, 4718, 4726, 4743, 4805, 4843, 4864, 5192, 5308, 5381, 5486, 5554, 5571, 5707, 5720, 5820, 5967, 6014, 6052, 6088, 6229, 6294, 6417, 6418, 6475, 6522, 6568, 6572, 6609, 6629, 6634, 6710, 6735, 6749, 6752, 6753, 6791, 6847, 6848, 6907, 6931, 6944, 6959, 6973, 7133]\n",
      "\n",
      "============ testing 'get_words_with_bigram' ====================================\n",
      "returned list length: 2121\n",
      "checking word هیلاندراس : True\n",
      "\n",
      "============ testing save and load methods ========================================\n",
      "length of posting list for word فکری before saving: 79\n",
      "number of ocurrences for  فکری : 169\n",
      "length of posting list for word فکری after loading: 79\n",
      "number of ocurrences for  فکری : 169\n"
     ]
    }
   ],
   "source": [
    "word1 = 'فکری'\n",
    "doc_id = 3014\n",
    "\n",
    "\n",
    "word2 = 'هیلاندراس'\n",
    "doc_id2 = 6752\n",
    "bigram = 'لا'\n",
    "\n",
    "def get_count (l):\n",
    "    i = [1 for _,t in l.items() for q in t['text']]\n",
    "    j = [1 for _,t in l.items() if 'title' in t.keys() for q in t['title']]\n",
    "    return len (i) + len(j)\n",
    "\n",
    "\n",
    "def test_prepare_text():\n",
    "    print (\"\\n============ testing 'prepare_text' =============================================\")\n",
    "    raw_text = \"کتابهای مناسبی نوشته شوند ! در راستای ارتقای . سطح آموزش کشور ؟ تلاش‌های زیادی صورت می‌گیرد\"\n",
    "    prepared_text = prepare_text(raw_text)\n",
    "    \n",
    "    print(\"prepared text is :\", prepared_text , \"with length:\" , len (prepared_text))\n",
    "    \n",
    "test_prepare_text()\n",
    "\n",
    "def test_get_posting_list():\n",
    "    \n",
    "    print (\"\\n============ testing 'get_posting_list' =========================================\")\n",
    "    \n",
    "    prepared_text = prepare_text(word1)[0]\n",
    "    posting_list = get_posting_list(prepared_text)\n",
    "    # posting_list = {3014:{'title':[...] , 'text':[...]}}\n",
    "    \n",
    "    \n",
    "#     print (\"posting list for input\" , prepared_text, \"is :\", posting_list , \"with length:\" , len (posting_list))\n",
    "    print (\"number of ocurrences of the word\", word1 , \" in documents = \", get_count (posting_list))\n",
    "    print ('docs with the word:' , sorted (list (posting_list.keys())))\n",
    "    \n",
    "test_get_posting_list()\n",
    "\n",
    "\n",
    "def test_bigram():\n",
    "    print (\"\\n============ testing 'get_words_with_bigram' ====================================\")\n",
    "    \n",
    "    words_with_bigram = get_words_with_bigram(bigram)\n",
    "    print (\"returned list length:\" , len (words_with_bigram))\n",
    "\n",
    "    print (\"checking word\" , word2 , \":\", word2 in words_with_bigram)\n",
    "    \n",
    "\n",
    "check_bigram = True\n",
    "if check_bigram:\n",
    "    test_bigram()\n",
    "def test_save_and_load ():\n",
    "    print (\"\\n============ testing save and load methods ========================================\")\n",
    "    \n",
    "    destination = 'project1_data\\\\data\\\\index_backup.json'\n",
    "\n",
    "    posting_list = get_posting_list(prepare_text(word1)[0])\n",
    "    print (\"length of posting list for word\" , word1 , \"before saving:\" , len (posting_list))\n",
    "    print (\"number of ocurrences for \", word1, \":\", get_count (posting_list))\n",
    "    \n",
    "    save_index(destination)\n",
    "    main_index = load_index(destination)\n",
    "    \n",
    "    posting_list = get_posting_list(prepare_text(word1)[0])\n",
    "    print (\"length of posting list for word\" , word1 , \"after loading:\" , len (posting_list))\n",
    "    print (\"number of ocurrences for \", word1, \":\", get_count (posting_list))\n",
    "    \n",
    "test_save_and_load ()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "C5JeSxd2Jnet"
   },
   "source": [
    "<div style=\"direction:rtl;line-height:300%;text-align:justify;\" align=\"justify\"><font face=\"XB Zar\" size=5>\n",
    "<font color=red size=7>\n",
    "<p></p>\n",
    "<div align=center> (40 نمره) بخش سوم: جستجو وبازیابی اسناد</div>\n",
    "</font>\n",
    "<hr>\n",
    "در این قسمت لازم است تا پرسمانی که از کاربر گرفته می‌شود در مجموعه اسناد نمایه شده، جستجو شود. جستجو به دو صورت بازیابی ترتیب دار در فضای برداری و بازیابی دقیق عبارت است. جستجو باید هم در عنوان سند صورت بگیرد هم در متن آن و در نهایت ترتیب اسناد بازگردانده شده بر اساس امتیازی‌ است که از جمع وزن‌دار امتیاز جست‌وجو در عنوان و جست‌وجو در متن به دست آمده‌است.\n",
    "(*امتیازی*)\n",
    "همچنین گاهی ممکن است پرسمان ارائه شده حاوی غلط املایی باشد، در این صورت لازم است تا ابتدا پرسمان را اصلاح کنید و سپس جستجو انجام شود. \n",
    "<br>\n",
    "<br>\n",
    "<font color=red size=6>\n",
    "(*امتیازی*)\n",
    "اصلاح پرسمان\n",
    "</font>\n",
    "<br>\n",
    "اصلاح پرسمان ورودی: ممکن است پرسمان ورودی\n",
    "کاربر غلط املایی داشته باشد؛ در چنین مواردی برای هر لغت از پرسمان ورودی که در نمایه موجود  نیست ابتدا نزدیکترین لغات موجود در نمایه \n",
    "bigram\n",
    "(با استفاده از معیار فاصله جاکارد) \n",
    "انتخاب شده و سپس\n",
    "بهترین آنها با معیار \n",
    "edit distance\n",
    "نسبت به کلمه اصلی، جایگزین می‌شود. در صورتی که چند لغت فاصله برابری از لغت مورد نظر داشته باشند، می‌توانید یکی از آنها را به دلخواه انتخاب کنید.\n",
    "<br>\n",
    "<br>\n",
    "<font color=red size=6>\n",
    "بازیابی ترتیب دار در فضای برداری tf-idf به روشهای ltn-lnn و ltc-lnc\n",
    "</font>\n",
    "<br>\n",
    " در این بخش پرسمان به صورت یک پرسمان کلی مطرح می‌شود و جست‌وجوی یک پرسمان بر روی هر دو بخش عنوان و متن صورت می‌گیرد و سپس نتیجه بر اساس امتیاز وزن‌دار جست و جو بر روی این دو بخش به ترتیب برگردانده می‌شود. وزن امتیاز جست‌وجو در عنوان نسبت به وزن امتیاز جست‌وجو در متن باید به عنوان پارامتر ورودی قابل تنظیم باشد اما در حالت پیش‌فرض آن را ۲ در نظر می‌گیریم. \n",
    "<br>\n",
    "برای هر پرسمان، پس از مشخص شدن روش امتیازدهی به عنوان ورودی\n",
    "(ltn-lnn\n",
    "و\n",
    "ltc-lnc)\n",
    "شما باید لیستی مرتب از شناسه اسناد بر اساس امتیاز کسب شده برگردانید که امتیازات بر اساس توضیحات بالا باید محاسبه شوند.\n",
    "<br>\n",
    "<br>\n",
    "<font color=red size=6>\n",
    "جستجوی دقیق \n",
    "(phrasal search)\n",
    "</font>\n",
    "<br>\n",
    "این نوع جست‌وجو در قالب جست‌وجو‌های ترتیب‌دار قسمت قبل استفاده می‌شود. به این ترتیب که \n",
    "پرسمان ورودی ممکن است شامل تعدادی لغت و عبارات داخل گیومه باشد. اسناد بازیابی شده می‌بایست شامل عبارات داخل گیومه دقیقا به همان ترتیب و شکل آمده داخل گیومه باشند. \n",
    "<br>\n",
    "در صورت وجود چند عبارت داخل گیومه در پرسمان، ترتیب عبارات آمده داخل چند گیومه نسبت به هم لزومی ندارد حفظ شود. به \n",
    "عنوان نمونه برای پرسمان\n",
    "<br>\n",
    "\"q5 q4\" q3 \"q2 q1\"\n",
    "<br>\n",
    "سند\n",
    "<br>\n",
    "q3 q2 q1 q5 q4\n",
    "<br>\n",
    "مرتبط محسوب می‌شود. \n",
    "<br>\n",
    "جست‌وجو باید به این صورت باشد که ابتدا مجموعەی تمامی اسناد دارای عبارت‌های داخل گیومه پیدا می‌شود و سپس با استفاده از تمام لغات داخل پرسمان (شامل لغات داخل گیومه) بازیابی ترتیب دار با توضیحات آمده در قسمت قبل انجام شود.\n",
    "</font></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "8UnpNX56Jneu"
   },
   "source": [
    "<div style=\"direction:rtl;line-height:300%;;text-align:justify;\" align=\"justify\"><font face=\"XB Zar\" size=5>\n",
    "(*امتیازی*)\n",
    "این بخش برای اصلاح پرسمان‌های ورودی است. تابع \n",
    "correct_query\n",
    "پرسمان کاربر  \n",
    "(query)\n",
    "را به عنوان ورودی می‌گیرد و در صورتی که کلماتی در پرسمان داخل واژه‌نامه‌ی نمایه وجود نداشته باشد آن کلمات را به شکل توضیح داده‌شده در بخش اصلاح پرسمان، با کلمات نزدیک به آن در واژه‌نامه جایگزین می‌کند و پرسمان اصلاح‌شده را برمی‌گرداند.\n",
    "</font>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "X5kybrGDJnev"
   },
   "outputs": [],
   "source": [
    "# def correct_query(query):\n",
    "#     correct_query = \"سلام حالا پرسمان درست شد.\"\n",
    "#     return correct_query\n",
    "\n",
    "def best_edit_distance(exact_word, new_words):\n",
    "    #این تابع کلمه با بهترین ادیت دیستنس (از بین یک سری کلمات) را از یک کلمه خاص بر میگرداند.\n",
    "    edit_distance = dict()\n",
    "    for w in new_words:\n",
    "        m = [[0 for i in range(len(exact_word) + 1)] for j in range(len(w) + 1)]\n",
    "        for i in range(len(w) + 1):\n",
    "            m[i][0] = i\n",
    "        for i in range(len(exact_word) + 1):\n",
    "            m[0][i] = i\n",
    "        for i in range(1, len(w) + 1):\n",
    "            for j in range(1, len(exact_word) + 1):\n",
    "                if w[i - 1] == exact_word[j - 1]:\n",
    "                    m[i][j] = m[i-1][j-1]\n",
    "                else:\n",
    "                    m[i][j] =  1 + min (m[i][j - 1], m[i - 1][j], m[i-1][j-1])\n",
    "        edit_distance[w] = m[len(w)][len(exact_word)]\n",
    "    return min(edit_distance, key=edit_distance.get)\n",
    "\n",
    "def give_bigrams(word):\n",
    "    #این تابع مجموعه دو حرفی های یک کلمه را برمیگرداند.\n",
    "    bigrams = set()\n",
    "    for i in range(0, len(word) - 1):\n",
    "        bigrams.add(word[i:i+2])\n",
    "    return bigrams\n",
    "\n",
    "\n",
    "def correct_query(query):\n",
    "    words = replace_bad_chars(query).split(\" \")\n",
    "    #words = prepare_text(query)\n",
    "    correct_words = []\n",
    "    for w in words:\n",
    "        if w not in main_index:\n",
    "            bigram_lists = []\n",
    "            for x in give_bigrams(w):\n",
    "                if x in bigram_index:\n",
    "                    bigram_lists.append(bigram_index[x])\n",
    "            intersect_whole_bigrams = dict()\n",
    "            for ls in bigram_lists:\n",
    "                for word in ls:\n",
    "                    if word in intersect_whole_bigrams:\n",
    "                        intersect_whole_bigrams[word][0] += 1\n",
    "                    else:\n",
    "                        intersect_whole_bigrams[word] = [1, len(word) - 1]\n",
    "            length_w = len(give_bigrams(w))\n",
    "            jaccard_distance = {x: intersect_whole_bigrams[x][0] / (len(give_bigrams(x)) + length_w - 2 - intersect_whole_bigrams[x][0])\n",
    "                                for x in intersect_whole_bigrams}\n",
    "            maximum = max(jaccard_distance.values())\n",
    "            closest_words = []\n",
    "            for x in jaccard_distance:\n",
    "                if jaccard_distance[x] == maximum:\n",
    "                    closest_words.append(x)\n",
    "            if len(closest_words) > 1:\n",
    "                best_word = best_edit_distance(w, closest_words)\n",
    "            else:\n",
    "                best_word = closest_words[0]\n",
    "            correct_words.append(best_word)\n",
    "        else:\n",
    "            correct_words.append(w)\n",
    "    return correct_words\n",
    "\n",
    "#correct_query(\"شلامت\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_correct_query():\n",
    "\n",
    "    ##################################\n",
    "    ## Do not change this part\n",
    "    ##################################\n",
    "    query = 'ابذار های فظایی و پیشرفته ناصا'\n",
    "    ##################################\n",
    "    \n",
    "    result = correct_query(query)\n",
    "    print(result)\n",
    "\n",
    "test_correct_query()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "T9HMqDMZJney"
   },
   "source": [
    "<div style=\"direction:rtl;line-height:300%;;text-align:justify;\" align=\"justify\"><font face=\"XB Zar\" size=5>\n",
    "این بخش به جست و جوی پرسمان کلی اختصاص دارد. تابع \n",
    "search\n",
    "به عنوان اولین پارامتر پرسمان \n",
    "(query)\n",
    "را گرفته و جست و جو را روی آن انجام می‌دهد.\n",
    "در صورتی که درون پرسمان بخشی داخل\n",
    "\"\"\n",
    "قرار گیرد به این معنی است که آن بخش باید به صورت جست‌وجوی دقیق در جست‌وجو در نظر گرفته‌شود. \n",
    "پارامتر دوم ورودی روش محاسبه امتیاز \n",
    "(method)\n",
    "است که می‌تواند یکی از دو مقدار\n",
    "ltn-lnn\n",
    "و\n",
    "ltc-lnc\n",
    "را بپذیرد که به طور پیش‌فرض مقدار اول را می‌پذیرد.\n",
    "پارامتر سوم \n",
    "(weight)\n",
    "که یک عدد اعشاری است نسبت وزن امتیاز جست‌وجو در عنوان به امتیاز جست‌وجو در متن را مشخص می‌کند. که به طور پیش‌فرض این مقدار برابر ۲ است. \n",
    "</font>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def detect_phrase(query):\n",
    "    #این تابع، با توجه به علامت گیومه ، در کوئری بررسی می کند که فریز وجود دارد یا خیر و تمام فریز ها را بر می گرداند.\n",
    "    result = set()\n",
    "    if '\"' in query:\n",
    "        start = 0\n",
    "        while query.find('\"',start) != -1:\n",
    "            t = query.find('\"', start)\n",
    "            f = query.find('\"', t+1)\n",
    "            result.add(query[t+1:f])\n",
    "            start = f+1\n",
    "    return result\n",
    "\n",
    "\n",
    "def intersection(words, text_title, ans_ids):\n",
    "    #این تابع تمام داک هایی که تمام کلمات یک فریز را دارند بر میگرداند\n",
    "    for w in words:\n",
    "        if len(ans_ids) == 0:\n",
    "            return {}\n",
    "        if w in main_index:\n",
    "            temp = set()\n",
    "            for k in main_index[w]:\n",
    "                if text_title == \"both\":\n",
    "                    temp.add(k)\n",
    "                elif text_title in main_index[w][k]:\n",
    "                    temp.add(k)\n",
    "            ans_ids = ans_ids & temp\n",
    "        else:\n",
    "            return {}\n",
    "    return ans_ids\n",
    "\n",
    "\n",
    "def check_has_phrase(words, ans_ids, text_title):\n",
    "    #این تابع چک می کند که ترتیب کلمات فریز داده شده در داک درست باشد.\n",
    "    answer = set()\n",
    "    if text_title != \"both\":\n",
    "        for doc in ans_ids:\n",
    "            places = set(main_index[words[0]][doc][text_title])\n",
    "            for i in range(1, len(words)):\n",
    "                temp = set()\n",
    "                for place in main_index[words[i]][doc][text_title]:\n",
    "                    if place - 1 in places:\n",
    "                        temp.add(place)\n",
    "                places = temp\n",
    "                if len(places) == 0:\n",
    "                    break\n",
    "            if len(places) > 0:\n",
    "                answer.add(doc)\n",
    "    else:\n",
    "        for doc in ans_ids:\n",
    "            title_places = set()\n",
    "            text_places = set()\n",
    "            if \"title\" in main_index[words[0]][doc]:\n",
    "                title_places = set(main_index[words[0]][doc][\"title\"])\n",
    "            if \"text\" in main_index[words[0]][doc]:\n",
    "                text_places = set(main_index[words[0]][doc][\"text\"])\n",
    "            for i in range(1, len(words)):\n",
    "                temp_title = set()\n",
    "                temp_text = set()\n",
    "                if \"title\" in main_index[words[i]][doc]:\n",
    "                    for place in main_index[words[i]][doc][\"title\"]:\n",
    "                        if place - 1 in title_places:\n",
    "                            temp_title.add(place)\n",
    "                if \"text\" in main_index[words[i]][doc]:\n",
    "                    for place in main_index[words[i]][doc][\"text\"]:\n",
    "                        if place - 1 in text_places:\n",
    "                            temp_text.add(place)\n",
    "                title_places = temp_title\n",
    "                text_places = temp_text\n",
    "                if len(title_places) == 0 and len(text_places) == 0:\n",
    "                    break;\n",
    "            if len(title_places) > 0 or len(text_places) > 0:\n",
    "                answer.add(doc)\n",
    "    return answer  \n",
    "    \n",
    "    \n",
    "def phrasal_search(ans_ids,phrases, text_title):\n",
    "    #این تابع جست و جوی دقیق را انجام می دهد.\n",
    "    for p in phrases:\n",
    "        words = prepare_text(p)\n",
    "        ans_ids = intersection(words, text_title, ans_ids)\n",
    "        if len(ans_ids) > 0 and len(words) > 1:\n",
    "            ans_ids = check_has_phrase(words, ans_ids, text_title)\n",
    "        if len(ans_ids) == 0:\n",
    "            return {}\n",
    "    return ans_ids\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "qsgz32PDJnez",
    "outputId": "b3b6ed43-794b-42e1-bc03-ee3e0134318b",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import heapq\n",
    "\n",
    "def give_df(term, text_title):\n",
    "    if term in main_index:\n",
    "        res = 0\n",
    "        for x in main_index[term]:\n",
    "            if text_title in main_index[term][x]:\n",
    "                res += 1\n",
    "        return res\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "\n",
    "def give_idf(term, text_title):\n",
    "    df = give_df(term, text_title)\n",
    "    if df == 0:\n",
    "        return 0\n",
    "    else:\n",
    "        return math.log10(len(doc_ids) / df)\n",
    "\n",
    "def query_log_tf(words):\n",
    "    keys = set(words)\n",
    "    ans = {k:0 for k in keys}\n",
    "    for w in words:\n",
    "        ans[w] += 1\n",
    "    for a in ans:\n",
    "        ans[a] = 1 + math.log10(ans[a])\n",
    "    return ans\n",
    "\n",
    "\n",
    "def doc_log_tf(doc, term, text_title):\n",
    "    if term in main_index:\n",
    "        if doc in main_index[term]:\n",
    "            if text_title in main_index[term][doc]:\n",
    "                return 1 + math.log10(len(main_index[term][doc][text_title]))\n",
    "            else:\n",
    "                return 0\n",
    "        else:\n",
    "            return 0\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "def normalize(dictionary):\n",
    "    #this function does cosine normalize.\n",
    "    s = sum(pow(dictionary[x],2) for x in dictionary)\n",
    "    if s > 0:\n",
    "        dictionary = {x: dictionary[x] / math.sqrt(s) for x in dictionary}\n",
    "    return dictionary\n",
    "\n",
    "def drop_zero_scores(title_query_words, text_query_words): \n",
    "    #this function returns docs that at least have one term in proper place.\n",
    "    answer = set()\n",
    "    for w in title_query_words:\n",
    "        if w in main_index:\n",
    "            for k in main_index[w]:\n",
    "                if \"title\" in main_index[w][k]:\n",
    "                    answer.add(k)\n",
    "    for w in text_query_words:\n",
    "        if w in main_index:\n",
    "            for k in main_index[w]:\n",
    "                if \"text\" in main_index[w][k]:\n",
    "                    answer.add(k)\n",
    "    return answer\n",
    "\n",
    "def calculate_scores(title_query_words, text_query_words, title_query_tfs, text_query_tfs, docs, method, weight):\n",
    "    title_idfs = {w: give_idf(w, \"title\") for w in title_query_words}\n",
    "    text_idfs = {w: give_idf(w, \"text\") for w in text_query_words}\n",
    "    results = []\n",
    "    for doc in docs:\n",
    "        doc_title_tfs = {w: doc_log_tf(doc, w, \"title\") for w in title_query_words}\n",
    "        doc_text_tfs = {w: doc_log_tf(doc, w, \"text\") for w in text_query_words}\n",
    "        doc_title_wts = {w: doc_title_tfs[w] * title_idfs[w] for w in title_query_words}\n",
    "        doc_text_wts = {w: doc_text_tfs[w] * text_idfs[w] for w in text_query_words}\n",
    "        if method == \"ltc-lnc\":\n",
    "            doc_title_wts = normalize(doc_title_wts)\n",
    "            doc_text_wts = normalize(doc_text_wts)\n",
    "        doc_title_scores = {w: doc_title_wts[w] * title_query_tfs[w] for w in title_query_words}\n",
    "        doc_text_scores = {w: doc_text_wts[w] * text_query_tfs[w] for w in text_query_words}\n",
    "        whole_title_score = sum(doc_title_scores[w] for w in title_query_words)\n",
    "        whole_text_score = sum(doc_text_scores[w] for w in text_query_words)\n",
    "        final_score = weight * whole_title_score + whole_text_score\n",
    "        heapq.heappush(results, (-final_score, doc))\n",
    "    return results\n",
    "\n",
    "\n",
    "def search(query, method=\"ltn-lnn\", weight=2):\n",
    "    phrases = detect_phrase(query)\n",
    "    docs = doc_ids\n",
    "    if len(phrases) > 0:\n",
    "        docs = phrasal_search(docs,phrases, \"both\")\n",
    "    words = prepare_text(query)\n",
    "    if len(docs) == len(doc_ids):\n",
    "        docs = drop_zero_scores(words, words)\n",
    "    #print(len(docs))\n",
    "    query_tfs = query_log_tf(words)\n",
    "    if method == \"ltc-lnc\":\n",
    "        query_tfs = normalize(query_tfs)\n",
    "    results_heap = calculate_scores(words, words, query_tfs, query_tfs, docs, method, weight)\n",
    "    relevant_docs = []\n",
    "    for i in range(min(len(results_heap), 15)):\n",
    "        relevant_docs.append(heapq.heappop(results_heap)[1])\n",
    "    return relevant_docs\n",
    "\n",
    "#search('\"نظرخواهی انجام شده توسط دانشگاه \"شهر نیویورک', \"ltc-lnc\", 3)\n",
    "#search('تاریخ علوم اجتماعی در اروپا')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[3903, 3415, 4624, 6629, 4376, 3411, 6266, 4659, 4627, 5724, 4255, 3414, 3416, 4573, 4259]\n"
     ]
    }
   ],
   "source": [
    "def test_search():\n",
    "\n",
    "    ##################################\n",
    "    ## Do not change this part\n",
    "    ##################################\n",
    "    query = 'سیاره های بزرگ \"منظومه شمسی\"'\n",
    "    method = \"ltc-lnc\"\n",
    "    ##################################\n",
    "\n",
    "    relevant_docs = search(query, method)\n",
    "    print(relevant_docs)\n",
    "\n",
    "test_search()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "fTXhfuP0Jne5"
   },
   "source": [
    "<div style=\"direction:rtl;line-height:300%;;text-align:justify;\" align=\"justify\"><font face=\"XB Zar\" size=5>\n",
    "این بخش به جست و جوی پرسمان بر اساس بخش اختصاص دارد. تابع \n",
    "detailed_search\n",
    "به عنوان دو پارامتر اول پرسمان بر روی عنوان \n",
    "(title_query)\n",
    "و پرسمان بر روی متن\n",
    "(text_query)\n",
    "را گرفته و جست و جو را روی آن‌ها انجام می‌دهد.\n",
    "در صورتی که درون پرسمان بخشی داخل\n",
    "\"\"\n",
    "قرار گیرد به این معنی است که آن بخش باید به صورت جست‌وجوی دقیق در جست‌وجو در نظر گرفته‌شود. \n",
    "پارامتر دوم ورودی روش محاسبه امتیاز \n",
    "(method)\n",
    "است که می‌تواند یکی از دو مقدار\n",
    "ltn-lnn\n",
    "و\n",
    "ltc-lnc\n",
    "را بپذیرد که به طور پیش‌فرض مقدار اول را می‌پذیرد. \n",
    "</font>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "BmOjg1gYJne6",
    "outputId": "88f5ed0f-3ac6-4bba-c148-8e7aa4de09c7"
   },
   "outputs": [],
   "source": [
    "def detailed_search(title_query, text_query, method=\"ltn-lnn\"):\n",
    "    title_phrases = detect_phrase(title_query)\n",
    "    text_phrases = detect_phrase(text_query)\n",
    "    docs = doc_ids\n",
    "    if len(title_phrases) > 0:\n",
    "        docs = phrasal_search(docs, title_phrases, \"title\")\n",
    "    if len(text_phrases) > 0:\n",
    "        docs = phrasal_search(docs, text_phrases, \"text\")\n",
    "    title_query_words = prepare_text(title_query)\n",
    "    text_query_words = prepare_text(text_query)\n",
    "    if len(docs) == len(doc_ids):\n",
    "        docs = drop_zero_scores(title_query_words, text_query_words)\n",
    "    title_query_tfs = query_log_tf(title_query_words)\n",
    "    text_query_tfs = query_log_tf(text_query_words)\n",
    "    if method == \"ltc-lnc\":\n",
    "        title_query_tfs = normalize(title_query_tfs)\n",
    "        text_query_tfs = normalize(text_query_tfs)\n",
    "    results_heap = calculate_scores(title_query_words, text_query_words, title_query_tfs, text_query_tfs, docs, method, 1)\n",
    "    relevant_docs = []\n",
    "    for i in range(min(len(results_heap), 15)):\n",
    "        relevant_docs.append(heapq.heappop(results_heap)[1])\n",
    "    return relevant_docs\n",
    "\n",
    "#detailed_search('عجایب هفت‌گانه', 'چشمگیرترین بناهای تاریخی جهان', \"ltc-lnc\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[3760, 5264, 5236, 3758, 6369, 3874, 5644, 6394, 4275, 4094, 5381, 6159, 6417, 4339, 5932]\n"
     ]
    }
   ],
   "source": [
    "def test_detailed_search():\n",
    "    \n",
    "    ##################################\n",
    "    ## Do not change this part\n",
    "    ##################################\n",
    "    title_query = 'فهرست شهرهای ایران'\n",
    "    text_query = 'استان گیلان شهرستان لنگرود'\n",
    "    ##################################\n",
    "\n",
    "    relevant_docs = detailed_search(title_query, text_query)\n",
    "    print(relevant_docs)\n",
    "\n",
    "test_detailed_search()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "jpAO-xvEJne-"
   },
   "source": [
    "<div style=\"direction:rtl;line-height:300%;text-align:justify;\" align=\"justify\"><font face=\"XB Zar\" size=5>\n",
    "<font color=red size=7>\n",
    "<p></p>\n",
    "<div align=center> (20 نمره) بخش چهارم: ارزیابی سیستم</div>\n",
    "</font>\n",
    "<hr>\n",
    "سیستم شما باید قادر باشد با استفاده از معیارهای\n",
    "<ol>\n",
    "<li>\n",
    "MAP\n",
    "</li>\n",
    "<li>\n",
    "F-Measure\n",
    "</li>\n",
    "<li>\n",
    "R-Precision\n",
    "</li>\n",
    "<li>\n",
    "NDCG\n",
    "</li>\n",
    "</ol>\n",
    "نتایج را ارزیابی کند. برای ارزیابی تعدادی پرسمان و نتایج آنها در اختیار شما قرار گرفته است که باید پاسخ سیستم‌تان به پرسمان ها را با نتایج متناظر هر پرسمان ارزیابی و مقایسه کنید. در صورتی که کل پرسمان در یک خط آمده بود به این معنی است که پرسمان کلی است و تابع\n",
    "search \n",
    "باید برای آن فراخوانی شود و در صورتی که پرسمان در  دو خط آمده بود، خط اول پرسمان عنوان و خط دوم پرسمان متن خواهد بود و باید تابع\n",
    "detailed_search\n",
    "را برای آن فراخوانی کنید و نتیجه را ارزیابی کنید.\n",
    "<br>\n",
    "توجه کنید که این چهار معیار را جداگانه و مستقل از بقیه نیز بتوانید حساب کنید. حداکثر سند بازیابی شده را ۱۵ قرار دهید.    \n",
    "<br>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "l52cf_R-Jne_"
   },
   "source": [
    "<div style=\"direction:rtl;line-height:300%;text-align:justify;\" align=\"justify\"><font face=\"XB Zar\" size=5>\n",
    "در قطعه کد بخش زیر به ازای هر معیار یک تابع آمده است که به عنوان ورودی شماره پرسمان را می‌گیرد و با خواندن پرسمان و لیست مرتب سند‌های مرتبط با آن از فایل‌های مربوطه، جستجوی پرسمان را با توجه به نوع پرسمان انجام می‌دهد، نتیجه را ارزیابی کرده و مقدار محاسبه شده معیار را بر می‌گرداند.\n",
    "در صورتی که در ورودی به جای شماره پرسمان رشته‌ی\n",
    "all\n",
    "آمده بود ارزیابی باید بر روی تمامی اسناد صورت گیرد و میانگین مقادیر معیار ازیابی برای همه پرسمان‌ها به عنوان خروجی برگردانده شود.\n",
    "</font>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "GqaOk4ESJnfA",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import glob\n",
    "import os\n",
    "from statistics import mean\n",
    "\n",
    "def read_files(query_relevance, id):\n",
    "    if id == 'all':\n",
    "        results = dict()\n",
    "        for file in glob.glob('project1_data\\\\data\\\\%s\\\\*.txt'%(query_relevance,)):\n",
    "            with open(file, 'r',encoding=\"utf8\") as next_file:\n",
    "                results[os.path.basename(file)] = next_file.read()\n",
    "    else:\n",
    "        with open('project1_data\\\\data\\\\%s\\\\%s.txt'%(query_relevance,id,),encoding=\"utf8\") as next_file:\n",
    "            results = {\"%s.txt\"%(id,):next_file.read()}\n",
    "    if query_relevance == \"relevance\":\n",
    "        for x in results:\n",
    "            temp = results[x].split(\",\")\n",
    "            for i in range(len(temp)):\n",
    "                temp[i] = int(temp[i].strip())\n",
    "            results[x] = temp\n",
    "    return results\n",
    "\n",
    "\n",
    "def R_Precision(query_id='all'):\n",
    "    queries = read_files(\"queries\", query_id)\n",
    "    relevant_results = read_files(\"relevance\", query_id)\n",
    "    result = dict()\n",
    "    for x in queries:\n",
    "        query = queries[x]\n",
    "        number_of_lines = len(query.splitlines())\n",
    "        if number_of_lines == 1:\n",
    "            my_results = search(query)\n",
    "        else:\n",
    "            temp = query.splitlines()\n",
    "            my_results = detailed_search(temp[0], temp[1])\n",
    "        #R = min(len(my_results), len(relevant_results[x]))\n",
    "        R = len(relevant_results[x])\n",
    "        real_results = set(relevant_results[x])\n",
    "        count = 0\n",
    "        for r in my_results:\n",
    "            if r in real_results:\n",
    "                count += 1\n",
    "        result[x] = count / R\n",
    "    return mean(result.values())\n",
    "\n",
    "\n",
    "def F_measure(query_id='all'):\n",
    "    queries = read_files(\"queries\", query_id)\n",
    "    relevant_results = read_files(\"relevance\", query_id)\n",
    "    result = dict()\n",
    "    for x in queries:\n",
    "        query = queries[x]\n",
    "        number_of_lines = len(query.splitlines())\n",
    "        if number_of_lines == 1:\n",
    "            my_results = search(query)\n",
    "        else:\n",
    "            temp = query.splitlines()\n",
    "            my_results = detailed_search(temp[0], temp[1])\n",
    "        retrieved_relevant = len(set(my_results) & set(relevant_results[x]))\n",
    "        retrieved = len(my_results)\n",
    "        relevant = len(relevant_results[x])\n",
    "        precision = retrieved_relevant / retrieved\n",
    "        recall = retrieved_relevant / relevant\n",
    "        result[x] = (2 * precision * recall) / (precision + recall)\n",
    "    return mean(result.values())\n",
    "\n",
    "def MAP(query_id='all'):\n",
    "    queries = read_files(\"queries\", query_id)\n",
    "    relevant_results = read_files(\"relevance\", query_id)\n",
    "    result = dict()\n",
    "    for x in queries:\n",
    "        query = queries[x]\n",
    "        number_of_lines = len(query.splitlines())\n",
    "        if number_of_lines == 1:\n",
    "            my_results = search(query)\n",
    "        else:\n",
    "            temp = query.splitlines()\n",
    "            my_results = detailed_search(temp[0], temp[1])\n",
    "        real_results = set(relevant_results[x])\n",
    "        relevant_retrieved = 0\n",
    "        precisions = []\n",
    "        for i in range(len(my_results)):\n",
    "            if my_results[i] in real_results:\n",
    "                relevant_retrieved += 1\n",
    "                precisions.append(relevant_retrieved / (i+1))\n",
    "        result[x] = mean(precisions) if len(precisions) > 0 else 0\n",
    "    return mean(result.values())\n",
    "\n",
    "def NDCG(query_id='all'):\n",
    "    queries = read_files(\"queries\", query_id)\n",
    "    relevant_results = read_files(\"relevance\", query_id)\n",
    "    result = dict()\n",
    "    for x in queries:\n",
    "        query = queries[x]\n",
    "        number_of_lines = len(query.splitlines())\n",
    "        if number_of_lines == 1:\n",
    "            my_results = search(query)\n",
    "        else:\n",
    "            temp = query.splitlines()\n",
    "            my_results = detailed_search(temp[0], temp[1])\n",
    "        max_dcg = sum(1 / math.log2(i+1) for i in range(1,len(my_results)))\n",
    "        max_dcg += 1\n",
    "        real_results = set(relevant_results[x])\n",
    "        dcg = 0\n",
    "        for i in range(len(my_results)):\n",
    "            if my_results[i] in real_results:\n",
    "                if i == 0:\n",
    "                    dcg += 1\n",
    "                else:\n",
    "                    dcg += (1 / math.log2(i+1))\n",
    "        result[x] = dcg / max_dcg\n",
    "    return mean(result.values())\n",
    "\n",
    "#R_Precision()\n",
    "#F_measure(1)\n",
    "#MAP(1)\n",
    "#NDCG(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------\n",
      "doc:\tall\n",
      "R_Precision:\t0.59\n",
      "F_measure  :\t0.54\n",
      "MAP        :\t0.63\n",
      "NDCG       :\t0.51\n",
      "------------------------------\n",
      "doc:\t1\n",
      "R_Precision:\t0.81\n",
      "F_measure  :\t0.84\n",
      "MAP        :\t0.86\n",
      "NDCG       :\t0.81\n",
      "------------------------------\n",
      "doc:\t2\n",
      "R_Precision:\t0.72\n",
      "F_measure  :\t0.79\n",
      "MAP        :\t0.83\n",
      "NDCG       :\t0.85\n",
      "------------------------------\n",
      "doc:\t3\n",
      "R_Precision:\t0.36\n",
      "F_measure  :\t0.43\n",
      "MAP        :\t0.37\n",
      "NDCG       :\t0.36\n"
     ]
    }
   ],
   "source": [
    "##################################\n",
    "## Do not change this part\n",
    "##################################\n",
    "test_docs = ['all', 1, 2, 3]\n",
    "functions = {'R_Precision':R_Precision, 'F_measure':F_measure, 'MAP':MAP, 'NDCG': NDCG}\n",
    "##################################\n",
    "\n",
    "for doc in test_docs:\n",
    "    print(\"{}\\ndoc:\\t{}\".format('-'*30, doc))\n",
    "    for f in functions.keys():\n",
    "        out = functions[f](doc)\n",
    "        print(\"{:11}:\\t{:.2f}\".format(f, out))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "rUs450l5JnfD"
   },
   "source": [
    "<div style=\"direction:rtl;line-height:300%;text-align:justify;\" align=\"justify\"><font face=\"XB Zar\" size=5>\n",
    "<font color=red size=7>\n",
    "<p></p>\n",
    "<div align=center>نکات پایانی</div>\n",
    "</font>\n",
    "<hr>\n",
    "۱- سیستم را به صورت بهینه پیاده سازی کنید تا در زمان کمتری بارگذاری و نمایه سازی و … را انجام دهد.\n",
    "<br>\n",
    "۲- فایل‌های \n",
    "ipynb\n",
    "و پایتون \n",
    "پاسخ تمرین را (بدون داده‌ها) به صورت فایل فشرده در کوئرا بارگذاری کنید.\n",
    "<br>\n",
    "۳- اشکالات خود از فاز اول پروژه را در زیر پست مربوط به این تمرین بپرسید.\n",
    "<br>\n",
    "۴- نام فایل ارسالی به صورت Project1-StudentNumber باشد.\n",
    "<br>\n",
    "۵- موعد تحویل تمرین تا ساعت ۲۳:۵۹ پانزدهم فروردین می‌باشد و جریمەی تأخیر مطابق با قوانینی که در سایت درس قرار داده شدەاست ، خواهد بود.\n",
    "<br>\n",
    "۶- در صورت مشاهده تقلب، طبق قوانین دانشکده با شما برخورد خواهد شد.\n",
    "<br>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------\n",
      "R_Precision:\n",
      "1:\t0.94\tFalse\n",
      "2:\t0.44\tTrue\n",
      "3:\t0.00\tTrue\n",
      "------------------------------\n",
      "MAP:\n",
      "1:\t1.00\tFalse\n",
      "2:\t1.00\tFalse\n",
      "3:\t0.00\tTrue\n",
      "------------------------------\n",
      "NDCG:\n",
      "1:\t1.00\tFalse\n",
      "2:\t1.00\tFalse\n",
      "3:\t0.00\tTrue\n"
     ]
    }
   ],
   "source": [
    "def detailed_search(title_query, text_query, method=\"ltn-lnn\"):\n",
    "    t = min(len(rels[idx]), 15)\n",
    "    return rels[idx][0:t]\n",
    "\n",
    "def search(query, method=\"ltn-lnn\", weight=2):\n",
    "    t = min(len(rels[idx]), 15)\n",
    "    return rels[idx][0:t]\n",
    "import glob\n",
    "import os, math\n",
    "from statistics import mean\n",
    "\n",
    "def read_files(query_relevance, id):\n",
    "    if id == 'all':\n",
    "        results = dict()\n",
    "        for file in glob.glob('project1_data\\\\data\\\\%s\\\\*.txt'%(query_relevance,)):\n",
    "            with open(file, 'r',encoding=\"utf8\") as next_file:\n",
    "                results[os.path.basename(file)] = next_file.read()\n",
    "    else:\n",
    "        with open('project1_data\\\\data\\\\%s\\\\%s.txt'%(query_relevance,id,),encoding=\"utf8\") as next_file:\n",
    "            results = {\"%s.txt\"%(id,):next_file.read()}\n",
    "    if query_relevance == \"relevance\":\n",
    "        for x in results:\n",
    "            temp = results[x].split(\",\")\n",
    "            for i in range(len(temp)):\n",
    "                temp[i] = int(temp[i].strip())\n",
    "            results[x] = temp\n",
    "    return results\n",
    "\n",
    "\n",
    "def R_Precision(query_id='all'):\n",
    "    queries = read_files(\"queries\", query_id)\n",
    "    relevant_results = read_files(\"relevance\", query_id)\n",
    "    result = dict()\n",
    "    for x in queries:\n",
    "        query = queries[x]\n",
    "        number_of_lines = len(query.splitlines())\n",
    "        if number_of_lines == 1:\n",
    "            my_results = search(query)\n",
    "        else:\n",
    "            temp = query.splitlines()\n",
    "            my_results = detailed_search(temp[0], temp[1])\n",
    "        #R = min(len(my_results), len(relevant_results[x]))\n",
    "        R = len(relevant_results[x])\n",
    "        real_results = set(relevant_results[x])\n",
    "        count = 0\n",
    "        for r in my_results:\n",
    "            if r in real_results:\n",
    "                count += 1\n",
    "        result[x] = count / R\n",
    "    return mean(result.values())\n",
    "\n",
    "\n",
    "def F_measure(query_id='all'):\n",
    "    queries = read_files(\"queries\", query_id)\n",
    "    relevant_results = read_files(\"relevance\", query_id)\n",
    "    result = dict()\n",
    "    for x in queries:\n",
    "        query = queries[x]\n",
    "        number_of_lines = len(query.splitlines())\n",
    "        if number_of_lines == 1:\n",
    "            my_results = search(query)\n",
    "        else:\n",
    "            temp = query.splitlines()\n",
    "            my_results = detailed_search(temp[0], temp[1])\n",
    "        retrieved_relevant = len(set(my_results) & set(relevant_results[x]))\n",
    "        retrieved = len(my_results)\n",
    "        relevant = len(relevant_results[x])\n",
    "        precision = retrieved_relevant / retrieved\n",
    "        recall = retrieved_relevant / relevant\n",
    "        result[x] = (2 * precision * recall) / (precision + recall)\n",
    "    return mean(result.values())\n",
    "\n",
    "def MAP(query_id='all'):\n",
    "    queries = read_files(\"queries\", query_id)\n",
    "    relevant_results = read_files(\"relevance\", query_id)\n",
    "    result = dict()\n",
    "    for x in queries:\n",
    "        query = queries[x]\n",
    "        number_of_lines = len(query.splitlines())\n",
    "        if number_of_lines == 1:\n",
    "            my_results = search(query)\n",
    "        else:\n",
    "            temp = query.splitlines()\n",
    "            my_results = detailed_search(temp[0], temp[1])\n",
    "        real_results = set(relevant_results[x])\n",
    "        relevant_retrieved = 0\n",
    "        precisions = []\n",
    "        for i in range(len(my_results)):\n",
    "            if my_results[i] in real_results:\n",
    "                relevant_retrieved += 1\n",
    "                precisions.append(relevant_retrieved / (i+1))\n",
    "        result[x] = mean(precisions) if len(precisions) > 0 else 0\n",
    "    return mean(result.values())\n",
    "\n",
    "def NDCG(query_id='all'):\n",
    "    queries = read_files(\"queries\", query_id)\n",
    "    relevant_results = read_files(\"relevance\", query_id)\n",
    "    result = dict()\n",
    "    for x in queries:\n",
    "        query = queries[x]\n",
    "        number_of_lines = len(query.splitlines())\n",
    "        if number_of_lines == 1:\n",
    "            my_results = search(query)\n",
    "        else:\n",
    "            temp = query.splitlines()\n",
    "            my_results = detailed_search(temp[0], temp[1])\n",
    "        max_dcg = sum(1 / math.log2(i+1) for i in range(1,len(my_results)))\n",
    "        max_dcg += 1\n",
    "        real_results = set(relevant_results[x])\n",
    "        dcg = 0\n",
    "        for i in range(len(my_results)):\n",
    "            if my_results[i] in real_results:\n",
    "                if i == 0:\n",
    "                    dcg += 1\n",
    "                else:\n",
    "                    dcg += (1 / math.log2(i+1))\n",
    "        result[x] = dcg / max_dcg\n",
    "    return mean(result.values())\n",
    "\n",
    "\n",
    "##################################\n",
    "## Do not change this part\n",
    "##################################\n",
    "test_docs = [1, 2, 3]\n",
    "rels = [\n",
    "    [6753, 7134, 6978, 7136, 4530, 6798, 6885, 5381, 6900, 4537, 5509, 6794, 4094, 6417, 3666, 5967],\n",
    "    [6753, 5509, 4718, 6798, 6850, 6417, 6978, 6871],\n",
    "    list(range(20))\n",
    "]\n",
    "outputs = [{'R_Precision': 1.0, 'F_measure': 0.967741935483871, 'MAP': 0.9375, 'NDCG': 0.9635640110263509},\n",
    "           {'R_Precision': 0.4444444444444444, 'F_measure': 0.6153846153846153, 'MAP': 0.4444444444444444, 'NDCG': 0.6313802022799658},\n",
    "           {'R_Precision': 0.0, 'F_measure': 0.0, 'MAP': 0.0, 'NDCG': 0.0}]\n",
    "\n",
    "functions = {'R_Precision':R_Precision, 'F_measure':F_measure, 'MAP':MAP, 'NDCG': NDCG}\n",
    "##################################\n",
    "idx = 0 \n",
    "\n",
    "ds = detailed_search\n",
    "s = search\n",
    "\n",
    "for f in functions.keys():\n",
    "    print(\"{}\\n{}:\".format('-'*30, f))\n",
    "    idx = 0\n",
    "    for doc in test_docs:    \n",
    "        out = functions[f](doc)\n",
    "        expected = outputs[idx][f]\n",
    "        print(\"{}:\\t{:.2f}\\t{}\".format(doc, out, abs(out-expected)<1e-3))\n",
    "        idx += 1\n",
    "\n",
    "detailed_search = ds\n",
    "search = s"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "x2AOY8hCJnfE"
   },
   "source": [
    "<div style=\"direction:rtl;line-height:300%;\"><font face=\"XB Zar\" size=5>\n",
    "<div align=center>\n",
    "<font face=\"B titr\" size=30>\n",
    "<p></p>\n",
    "<font color=#FF7500> \n",
    "موفق باشید\n",
    ":)\n",
    "<br>\n",
    "\n",
    "</font>\n",
    "</div>\n",
    "</font>\n",
    "</div>"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "colab": {
   "collapsed_sections": [],
   "name": "MIRProjectPhase1Spring99_ژخحغ.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
